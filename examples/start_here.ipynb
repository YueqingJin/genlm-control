{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 22:41:44 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    ")  # For sentiment analysis.\n",
    "\n",
    "from genlm_control import InferenceEngine\n",
    "from genlm_control.potential import PromptedLLM, BoolFSA, Potential\n",
    "from genlm_control.sampler import direct_token_sampler, eager_token_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task cancelling name='Task-528' coro=<AsyncTokenCharacterTrie._background_loop() running at /home/mila/b/benjamin.lebrun/scratch/genlm-control/lib/python3.11/site-packages/genlm_backend/trie/async_impl.py:107> wait_for=<Future cancelled>>\n",
      "/home/mila/b/benjamin.lebrun/scratch/genlm-control/lib/python3.11/site-packages/genlm_backend/tokenization/vocab.py:99: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load gpt2 (or any other HuggingFace model) using the HuggingFace backend.\n",
    "# (Setting backend='vllm' will be much faster, but requires a GPU).\n",
    "mtl_llm = PromptedLLM.from_name(\"gpt2\", backend=\"hf\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the fixed prompt prefix for the language model.\n",
    "# All language model predictions will be conditioned on the\n",
    "# token ids which this string encodes to (via the LM's tokenizer).\n",
    "mtl_llm.set_prompt_from_str(\"Montreal is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sampler that proposes tokens by sampling directly\n",
    "# from the language model's distribution.\n",
    "sampler = direct_token_sampler(mtl_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inference engine.\n",
    "engine = InferenceEngine(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SMC with 10 particles, a max sequence length of 25 tokens\n",
    "# and an ESS threshold of 0.5.\n",
    "sequences = await engine(n_particles=10, max_tokens=10, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>(b&#x27; not&#x27;, b&#x27; a&#x27;, b&#x27; city&#x27;, b&#x27; that&#x27;, b&#x27; has&#x27;, b&#x27; a&#x27;, b&#x27; reputation&#x27;, b&#x27; for&#x27;, b&#x27; being&#x27;, b&#x27; a&#x27;)</pre></td><td><pre>0.10000089820812255</pre></td> </tr><tr><td><pre>(b&#x27; home&#x27;, b&#x27; to&#x27;, b&#x27; a&#x27;, b&#x27; team&#x27;, b&#x27; of&#x27;, b&#x27; professional&#x27;, b&#x27; soccer&#x27;, b&#x27; players&#x27;, b&#x27;,&#x27;, b&#x27; including&#x27;)</pre></td><td><pre>0.10000035213773735</pre></td> </tr><tr><td><pre>(b&#x27; a&#x27;, b&#x27; city&#x27;, b&#x27; of&#x27;, b&#x27; people&#x27;, b&#x27; who&#x27;, b&#x27; have&#x27;, b&#x27; a&#x27;, b&#x27; strong&#x27;, b&#x27; sense&#x27;, b&#x27; of&#x27;)</pre></td><td><pre>0.10000033458578937</pre></td> </tr><tr><td><pre>(b&#x27; an&#x27;, b&#x27; urban&#x27;, b&#x27; area&#x27;, b&#x27;.&#x27;, b&#x27; The&#x27;, b&#x27; city&#x27;, b&#x27; is&#x27;, b&#x27; a&#x27;, b&#x27; pioneer&#x27;, b&#x27; of&#x27;)</pre></td><td><pre>0.10000022899048969</pre></td> </tr><tr><td><pre>(b&#x27; home&#x27;, b&#x27; to&#x27;, b&#x27; a&#x27;, b&#x27; number&#x27;, b&#x27; of&#x27;, b&#x27; famous&#x27;, b&#x27; movie&#x27;, b&#x27; stars&#x27;, b&#x27;,&#x27;, b&#x27; including&#x27;)</pre></td><td><pre>0.10000021817279635</pre></td> </tr><tr><td><pre>(b&#x27; the&#x27;, b&#x27; only&#x27;, b&#x27; city&#x27;, b&#x27; in&#x27;, b&#x27; Canada&#x27;, b&#x27; where&#x27;, b&#x27; you&#x27;, b&#x27; can&#x27;, b&#x27; still&#x27;, b&#x27; buy&#x27;)</pre></td><td><pre>0.0999997843072465</pre></td> </tr><tr><td><pre>(b&#x27; also&#x27;, b&#x27; the&#x27;, b&#x27; most&#x27;, b&#x27; populous&#x27;, b&#x27; city&#x27;, b&#x27; in&#x27;, b&#x27; France&#x27;, b&#x27;,&#x27;, b&#x27; with&#x27;, b&#x27; more&#x27;)</pre></td><td><pre>0.09999967070703746</pre></td> </tr><tr><td><pre>(b&#x27; a&#x27;, b&#x27; city&#x27;, b&#x27; with&#x27;, b&#x27; a&#x27;, b&#x27; population&#x27;, b&#x27; of&#x27;, b&#x27; over&#x27;, b&#x27; 6&#x27;, b&#x27; million&#x27;, b&#x27; people&#x27;)</pre></td><td><pre>0.09999964391270273</pre></td> </tr><tr><td><pre>(b&#x27; facing&#x27;, b&#x27; a&#x27;, b&#x27; budget&#x27;, b&#x27; shortfall&#x27;, b&#x27; of&#x27;, b&#x27; $&#x27;, b&#x27;8&#x27;, b&#x27;.&#x27;, b&#x27;6&#x27;, b&#x27; billion&#x27;)</pre></td><td><pre>0.09999944046738002</pre></td> </tr><tr><td><pre>(b&#x27; one&#x27;, b&#x27; of&#x27;, b&#x27; the&#x27;, b&#x27; most&#x27;, b&#x27; beautiful&#x27;, b&#x27; places&#x27;, b&#x27; in&#x27;, b&#x27; Canada&#x27;, b&#x27;,&#x27;, b&#x27; but&#x27;)</pre></td><td><pre>0.0999994285106981</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{(b' not', b' a', b' city', b' that', b' has', b' a', b' reputation', b' for', b' being', b' a'): 0.10000089820812255, (b' home', b' to', b' a', b' team', b' of', b' professional', b' soccer', b' players', b',', b' including'): 0.10000035213773735, (b' a', b' city', b' of', b' people', b' who', b' have', b' a', b' strong', b' sense', b' of'): 0.10000033458578937, (b' an', b' urban', b' area', b'.', b' The', b' city', b' is', b' a', b' pioneer', b' of'): 0.10000022899048969, (b' home', b' to', b' a', b' number', b' of', b' famous', b' movie', b' stars', b',', b' including'): 0.10000021817279635, (b' the', b' only', b' city', b' in', b' Canada', b' where', b' you', b' can', b' still', b' buy'): 0.0999997843072465, (b' also', b' the', b' most', b' populous', b' city', b' in', b' France', b',', b' with', b' more'): 0.09999967070703746, (b' a', b' city', b' with', b' a', b' population', b' of', b' over', b' 6', b' million', b' people'): 0.09999964391270273, (b' facing', b' a', b' budget', b' shortfall', b' of', b' $', b'8', b'.', b'6', b' billion'): 0.09999944046738002, (b' one', b' of', b' the', b' most', b' beautiful', b' places', b' in', b' Canada', b',', b' but'): 0.0999994285106981}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the inferred posterior distribution over sequences.\n",
    "sequences.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn a new language model. This is shallow copy, so both models\n",
    "# share the same underlying language model.\n",
    "bos_llm = mtl_llm.spawn()\n",
    "# Set a different prompt for the new language model.\n",
    "bos_llm.set_prompt_from_str(\"Boston is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the product of the two language models.\n",
    "product = mtl_llm * bos_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a token sampler that samples next tokens directly from the\n",
    "# product of the two language models.\n",
    "sampler = direct_token_sampler(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inference engine.\n",
    "engine = InferenceEngine(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference engine for 10 particles with a max sequence length of 25 tokens\n",
    "# and an ESS threshold of 0.5.\n",
    "sequences = await engine(n_particles=10, max_tokens=10, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>(b&#x27; a&#x27;, b&#x27; great&#x27;, b&#x27; place&#x27;, b&#x27; to&#x27;, b&#x27; live&#x27;, b&#x27;.&#x27;, b&#x27; It&#x27;, b&quot;&#x27;s&quot;, b&#x27; a&#x27;, b&#x27; great&#x27;)</pre></td><td><pre>0.6</pre></td> </tr><tr><td><pre>(b&#x27; a&#x27;, b&#x27; small&#x27;, b&#x27; town&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; middle&#x27;, b&#x27; of&#x27;, b&#x27; nowhere&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;)</pre></td><td><pre>0.29999999999999993</pre></td> </tr><tr><td><pre>(b&#x27; a&#x27;, b&#x27; great&#x27;, b&#x27; place&#x27;, b&#x27; to&#x27;, b&#x27; live&#x27;, b&#x27;.&#x27;, b&#x27; I&#x27;, b&quot;&#x27;m&quot;, b&#x27; looking&#x27;, b&#x27; forward&#x27;)</pre></td><td><pre>0.09999999999999999</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{(b' a', b' great', b' place', b' to', b' live', b'.', b' It', b\"'s\", b' a', b' great'): 0.6, (b' a', b' small', b' town', b' in', b' the', b' middle', b' of', b' nowhere', b',', b' and'): 0.29999999999999993, (b' a', b' great', b' place', b' to', b' live', b'.', b' I', b\"'m\", b' looking', b' forward'): 0.09999999999999999}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a regex constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fsa = BoolFSA.from_regex(r\"is\\sthe\\s(best|worst).*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is valid but will be slow!\n",
    "# slow_sampler = direct_token_sampler(\n",
    "#    product * best_fsa.coerce(product, f=b''.join)\n",
    "# )\n",
    "\n",
    "# This sampler is much faster.\n",
    "sampler = eager_token_sampler(product, best_fsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = InferenceEngine(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = await engine(n_particles=10, max_tokens=20, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&#x27; are&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&#x27; are&#x27;, b&#x27; the&#x27;)</pre></td><td><pre>0.8700819897729549</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; place&#x27;, b&#x27; to&#x27;, b&#x27; start&#x27;, b&#x27;.&#x27;, b&#x27; It&#x27;, b&quot;&#x27;s&quot;, b&#x27; a&#x27;, b&#x27; great&#x27;, b&#x27; place&#x27;, b&#x27; to&#x27;, b&#x27; start&#x27;, b&#x27;.&#x27;, b&#x27; It&#x27;, b&quot;&#x27;s&quot;, b&#x27; a&#x27;, b&#x27; great&#x27;, b&#x27; place&#x27;)</pre></td><td><pre>0.12595754038129175</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;, b&#x27; the&#x27;, b&#x27; team&#x27;, b&#x27; that&#x27;, b&#x27; has&#x27;, b&#x27; been&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27; for&#x27;)</pre></td><td><pre>0.002369224994716196</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;, b&#x27; it&#x27;, b&quot;&#x27;s&quot;, b&#x27; hard&#x27;, b&#x27; to&#x27;, b&#x27; believe&#x27;, b&#x27; that&#x27;, b&#x27; the&#x27;, b&#x27; team&#x27;, b&#x27; will&#x27;, b&#x27; be&#x27;, b&#x27; able&#x27;)</pre></td><td><pre>0.0007141910328063171</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;, b&#x27; it&#x27;, b&quot;&#x27;s&quot;, b&#x27; hard&#x27;, b&#x27; to&#x27;, b&#x27; believe&#x27;, b&#x27; that&#x27;, b&#x27; they&#x27;, b&quot;&#x27;re&quot;, b&#x27; going&#x27;, b&#x27; to&#x27;, b&#x27; be&#x27;)</pre></td><td><pre>0.0005464385592377434</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;, b&#x27; it&#x27;, b&quot;&#x27;s&quot;, b&#x27; hard&#x27;, b&#x27; to&#x27;, b&#x27; argue&#x27;, b&#x27; with&#x27;, b&#x27; that&#x27;, b&#x27;.&#x27;, b&#x27; The&#x27;, b&#x27; team&#x27;, b&#x27; is&#x27;)</pre></td><td><pre>0.0003306152589931078</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{(b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b' are', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b' are', b' the'): 0.8700819897729549, (b'is', b' the', b' best', b' place', b' to', b' start', b'.', b' It', b\"'s\", b' a', b' great', b' place', b' to', b' start', b'.', b' It', b\"'s\", b' a', b' great', b' place'): 0.12595754038129175, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' and', b' the', b' team', b' that', b' has', b' been', b' the', b' best', b' in', b' the', b' league', b' for'): 0.002369224994716196, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' and', b' it', b\"'s\", b' hard', b' to', b' believe', b' that', b' the', b' team', b' will', b' be', b' able'): 0.0007141910328063171, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' and', b' it', b\"'s\", b' hard', b' to', b' believe', b' that', b' they', b\"'re\", b' going', b' to', b' be'): 0.0005464385592377434, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' and', b' it', b\"'s\", b' hard', b' to', b' argue', b' with', b' that', b'.', b' The', b' team', b' is'): 0.0003306152589931078}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticizing with a custom `Potential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom potential that does sentiment analysis.\n",
    "\n",
    "\n",
    "class SentimentAnalysis(Potential):\n",
    "    def __init__(self, model, tokenizer, sentiment=\"POSITIVE\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentiment_idx = model.config.label2id.get(sentiment, None)\n",
    "        if self.sentiment_idx is None:\n",
    "            raise ValueError(f\"Sentiment {sentiment} not found in model labels\")\n",
    "\n",
    "        super().__init__(vocabulary=list(range(256)))  # Defined over bytes.\n",
    "\n",
    "    def _forward(self, contexts):\n",
    "        strings = [\n",
    "            bytes(context).decode(\"utf-8\", errors=\"ignore\") for context in contexts\n",
    "        ]  # Convert bytes to strings.\n",
    "        inputs = self.tokenizer(strings, return_tensors=\"pt\", padding=True)  # Tokenize.\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        return logits.log_softmax(dim=-1)[:, self.sentiment_idx].cpu().numpy()\n",
    "\n",
    "    async def prefix(self, context):\n",
    "        return self._forward([context])[0].item()\n",
    "\n",
    "    async def complete(self, context):\n",
    "        return self._forward([context])[0].item()\n",
    "\n",
    "    async def batch_complete(self, contexts):\n",
    "        return self._forward(contexts)\n",
    "\n",
    "    async def batch_prefix(self, contexts):\n",
    "        return self._forward(contexts)\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "sentiment_analysis = SentimentAnalysis(\n",
    "    model=DistilBertForSequenceClassification.from_pretrained(model_name),\n",
    "    tokenizer=DistilBertTokenizer.from_pretrained(model_name),\n",
    "    sentiment=\"POSITIVE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.00015841660206206143, -8.44865894317627)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await sentiment_analysis.prefix(b\"so good\"), await sentiment_analysis.prefix(b\"so bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that our custom potential satisfies the potential contract.\n",
    "await sentiment_analysis.assert_logw_next_consistency(b\"the best\", top=5)\n",
    "await sentiment_analysis.assert_autoreg_fact(b\"the best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task cancelling name='Task-1848' coro=<AsyncTokenCharacterTrie._background_loop() running at /home/mila/b/benjamin.lebrun/scratch/genlm-control/lib/python3.11/site-packages/genlm_backend/trie/async_impl.py:107> wait_for=<Future cancelled>>\n"
     ]
    }
   ],
   "source": [
    "# The following is valid but will be slow!\n",
    "# slow_sampler = eager_token_sampler(\n",
    "#    iter_potential=product, item_potential=best_fsa * sentiment_analysis\n",
    "# )\n",
    "\n",
    "# This setup will be much faster.\n",
    "sampler = eager_token_sampler(product, best_fsa)\n",
    "critic = sentiment_analysis.coerce(sampler.target, f=b\"\".join)\n",
    "engine = InferenceEngine(sampler, critic=critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = await engine(n_particles=10, max_tokens=10, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing with autobatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a new potential that automatically batches concurrent\n",
    "# requests to the instance methods (`prefix`, `complete`, `logw_next`)\n",
    "# and processes them using the batch methods (`batch_complete`, `batch_prefix`, `batch_logw_next`).\n",
    "critic = critic.to_autobatched()\n",
    "engine = InferenceEngine(sampler, critic=critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = await engine(n_particles=10, max_tokens=10, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; but&#x27;, b&#x27; they&#x27;)</pre></td><td><pre>0.3356544170239431</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; The&#x27;, b&#x27; only&#x27;)</pre></td><td><pre>0.1458168939011587</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&#x27; are&#x27;)</pre></td><td><pre>0.12042975039630092</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&#x27; have&#x27;)</pre></td><td><pre>0.12042961370049911</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; but&#x27;, b&#x27; it&#x27;)</pre></td><td><pre>0.1118988828314853</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;, b&#x27; they&#x27;)</pre></td><td><pre>0.09623392011459835</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27; and&#x27;, b&#x27; has&#x27;, b&#x27; the&#x27;)</pre></td><td><pre>0.06355272991534008</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; place&#x27;, b&#x27; to&#x27;, b&#x27; be&#x27;, b&#x27; a&#x27;, b&#x27; kid&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;)</pre></td><td><pre>0.0059837921166744435</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{(b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' but', b' they'): 0.3356544170239431, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' The', b' only'): 0.1458168939011587, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b' are'): 0.12042975039630092, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b' have'): 0.12042961370049911, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' but', b' it'): 0.1118988828314853, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' and', b' they'): 0.09623392011459835, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b' and', b' has', b' the'): 0.06355272991534008, (b'is', b' the', b' best', b' place', b' to', b' be', b' a', b' kid', b' in', b' the'): 0.0059837921166744435}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criticizing with a CPU-intensive potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
