{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genlm_control import InferenceEngine\n",
    "from genlm_control.potential import PromptedLLM, BoolFSA, Potential\n",
    "from genlm_control.sampler import direct_token_sampler, eager_token_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-26 08:49:15 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/b/benjamin.lebrun/scratch/genlm-control/lib/python3.11/site-packages/genlm_backend/tokenization/vocab.py:99: UserWarning: Duplicate tokens found in string vocabulary. This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load gpt2 (or any other HuggingFace model) using the HuggingFace backend.\n",
    "# (Setting backend='vllm' will be much faster, but requires a GPU).\n",
    "mtl_llm = PromptedLLM.from_name(\"gpt2\", backend=\"hf\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the fixed prompt prefix for the language model.\n",
    "# All language model predictions will be conditioned on the\n",
    "# token ids which this string encodes to (via the LM's tokenizer).\n",
    "mtl_llm.set_prompt_from_str(\"Montreal is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sampler that proposes tokens by sampling directly\n",
    "# from the language model's distribution.\n",
    "sampler = direct_token_sampler(mtl_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inference engine.\n",
    "engine = InferenceEngine(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SMC with 10 particles, a max sequence length of 25 tokens\n",
    "# and an ESS threshold of 0.5.\n",
    "sequences = await engine(n_particles=10, max_tokens=10, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>(b&#x27; a&#x27;, b&#x27; city&#x27;, b&#x27; of&#x27;, b&#x27; a&#x27;, b&#x27; thousand&#x27;, b&#x27; people&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;, b&#x27; it&#x27;, b&quot;&#x27;s&quot;)</pre></td><td><pre>0.10000062587195321</pre></td> </tr><tr><td><pre>(b&#x27; set&#x27;, b&#x27; to&#x27;, b&#x27; launch&#x27;, b&#x27; a&#x27;, b&#x27; new&#x27;, b&#x27; tech&#x27;, b&#x27; lab&#x27;, b&#x27; in&#x27;, b&#x27; its&#x27;, b&#x27; own&#x27;)</pre></td><td><pre>0.10000034590320922</pre></td> </tr><tr><td><pre>(b&#x27; a&#x27;, b&#x27; city&#x27;, b&#x27; with&#x27;, b&#x27; a&#x27;, b&#x27; high&#x27;, b&#x27; number&#x27;, b&#x27; of&#x27;, b&#x27; businesses&#x27;, b&#x27;,&#x27;, b&#x27; but&#x27;)</pre></td><td><pre>0.10000022453141078</pre></td> </tr><tr><td><pre>(b&#x27; about&#x27;, b&#x27; to&#x27;, b&#x27; get&#x27;, b&#x27; a&#x27;, b&#x27; new&#x27;, b&#x27; name&#x27;, b&#x27; for&#x27;, b&#x27; itself&#x27;, b&#x27;.&#x27;, b&#x27; The&#x27;)</pre></td><td><pre>0.1000001402314081</pre></td> </tr><tr><td><pre>(b&#x27; a&#x27;, b&#x27; city&#x27;, b&#x27; of&#x27;, b&#x27; about&#x27;, b&#x27; 5&#x27;, b&#x27; million&#x27;, b&#x27; people&#x27;, b&#x27;.&#x27;, b&#x27; It&#x27;, b&#x27; is&#x27;)</pre></td><td><pre>0.10000006956968781</pre></td> </tr><tr><td><pre>(b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; city&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; world&#x27;, b&#x27; for&#x27;, b&#x27; young&#x27;, b&#x27; people&#x27;, b&#x27;.&#x27;)</pre></td><td><pre>0.09999994869131731</pre></td> </tr><tr><td><pre>(b&#x27; one&#x27;, b&#x27; of&#x27;, b&#x27; the&#x27;, b&#x27; few&#x27;, b&#x27; cities&#x27;, b&#x27; in&#x27;, b&#x27; Canada&#x27;, b&#x27; that&#x27;, b&#x27; offers&#x27;, b&#x27; a&#x27;)</pre></td><td><pre>0.09999975226188743</pre></td> </tr><tr><td><pre>(b&#x27; the&#x27;, b&#x27; first&#x27;, b&#x27; city&#x27;, b&#x27; in&#x27;, b&#x27; Canada&#x27;, b&#x27; to&#x27;, b&#x27; ban&#x27;, b&#x27; smoking&#x27;, b&#x27; in&#x27;, b&#x27; public&#x27;)</pre></td><td><pre>0.099999743833424</pre></td> </tr><tr><td><pre>(b&#x27; one&#x27;, b&#x27; of&#x27;, b&#x27; the&#x27;, b&#x27; most&#x27;, b&#x27; popular&#x27;, b&#x27; destinations&#x27;, b&#x27; in&#x27;, b&#x27; Quebec&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;)</pre></td><td><pre>0.09999965745613204</pre></td> </tr><tr><td><pre>(b&#x27; one&#x27;, b&#x27; of&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; cities&#x27;, b&#x27; in&#x27;, b&#x27; Canada&#x27;, b&#x27; to&#x27;, b&#x27; live&#x27;, b&#x27;.&#x27;)</pre></td><td><pre>0.09999949164956994</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{(b' a', b' city', b' of', b' a', b' thousand', b' people', b',', b' and', b' it', b\"'s\"): 0.10000062587195321, (b' set', b' to', b' launch', b' a', b' new', b' tech', b' lab', b' in', b' its', b' own'): 0.10000034590320922, (b' a', b' city', b' with', b' a', b' high', b' number', b' of', b' businesses', b',', b' but'): 0.10000022453141078, (b' about', b' to', b' get', b' a', b' new', b' name', b' for', b' itself', b'.', b' The'): 0.1000001402314081, (b' a', b' city', b' of', b' about', b' 5', b' million', b' people', b'.', b' It', b' is'): 0.10000006956968781, (b' the', b' best', b' city', b' in', b' the', b' world', b' for', b' young', b' people', b'.'): 0.09999994869131731, (b' one', b' of', b' the', b' few', b' cities', b' in', b' Canada', b' that', b' offers', b' a'): 0.09999975226188743, (b' the', b' first', b' city', b' in', b' Canada', b' to', b' ban', b' smoking', b' in', b' public'): 0.099999743833424, (b' one', b' of', b' the', b' most', b' popular', b' destinations', b' in', b' Quebec', b',', b' and'): 0.09999965745613204, (b' one', b' of', b' the', b' best', b' cities', b' in', b' Canada', b' to', b' live', b'.'): 0.09999949164956994}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the inferred posterior distribution over sequences.\n",
    "sequences.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn a new language model. This is shallow copy, so both models\n",
    "# share the same underlying language model.\n",
    "bos_llm = mtl_llm.spawn()\n",
    "# Set a different prompt for the new language model.\n",
    "bos_llm.set_prompt_from_str(\"Boston is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the product of the two language models.\n",
    "product = mtl_llm * bos_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a token sampler that samples next tokens directly from the\n",
    "# product of the two language models.\n",
    "sampler = direct_token_sampler(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inference engine.\n",
    "engine = InferenceEngine(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference engine for 10 particles with a max sequence length of 25 tokens\n",
    "# and an ESS threshold of 0.5.\n",
    "sequences = await engine(n_particles=10, max_tokens=10, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>(b&#x27; the&#x27;, b&#x27; only&#x27;, b&#x27; city&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; country&#x27;, b&#x27; that&#x27;, b&#x27; doesn&#x27;, b&quot;&#x27;t&quot;, b&#x27; have&#x27;)</pre></td><td><pre>0.5</pre></td> </tr><tr><td><pre>(b&#x27; a&#x27;, b&#x27; great&#x27;, b&#x27; place&#x27;, b&#x27; to&#x27;, b&#x27; live&#x27;, b&#x27;.&#x27;, b&#x27; It&#x27;, b&quot;&#x27;s&quot;, b&#x27; a&#x27;, b&#x27; great&#x27;)</pre></td><td><pre>0.39999999999999997</pre></td> </tr><tr><td><pre>(b&#x27; a&#x27;, b&#x27; city&#x27;, b&#x27; of&#x27;, b&#x27; about&#x27;, b&#x27; 1&#x27;, b&#x27;.&#x27;, b&#x27;3&#x27;, b&#x27; million&#x27;, b&#x27; people&#x27;, b&#x27;,&#x27;)</pre></td><td><pre>0.09999999999999999</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{(b' the', b' only', b' city', b' in', b' the', b' country', b' that', b' doesn', b\"'t\", b' have'): 0.5, (b' a', b' great', b' place', b' to', b' live', b'.', b' It', b\"'s\", b' a', b' great'): 0.39999999999999997, (b' a', b' city', b' of', b' about', b' 1', b'.', b'3', b' million', b' people', b','): 0.09999999999999999}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a regex constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fsa = BoolFSA.from_regex(r\"is\\sthe\\s(best|worst).*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/b/benjamin.lebrun/scratch/genlm-control/lib/python3.11/site-packages/genlm_backend/trie/parallel.py:63: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  ).to_sparse_csr()\n"
     ]
    }
   ],
   "source": [
    "# The following is valid but will be slow!\n",
    "# slow_sampler = direct_token_sampler(\n",
    "#    product * best_fsa.coerce(product, f=b''.join)\n",
    "# )\n",
    "\n",
    "# This sampler is much faster.\n",
    "sampler = eager_token_sampler(product, best_fsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = InferenceEngine(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = await engine(n_particles=10, max_tokens=10, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&#x27; have&#x27;)</pre></td><td><pre>0.4888266356714295</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&quot;&#x27;ve&quot;)</pre></td><td><pre>0.24441333869533186</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; The&#x27;, b&#x27; team&#x27;)</pre></td><td><pre>0.1479736079456642</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; but&#x27;, b&#x27; they&#x27;)</pre></td><td><pre>0.11356924044156694</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27;-&#x27;, b&#x27;known&#x27;, b&#x27;,&#x27;, b&#x27; most&#x27;, b&#x27; successful&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;)</pre></td><td><pre>0.004418511235382777</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27;-&#x27;, b&#x27;known&#x27;, b&#x27; Canadian&#x27;, b&#x27; team&#x27;, b&#x27; to&#x27;, b&#x27; have&#x27;, b&#x27; won&#x27;)</pre></td><td><pre>0.000798666010624802</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{(b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b' have'): 0.4888266356714295, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b\"'ve\"): 0.24441333869533186, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' The', b' team'): 0.1479736079456642, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' but', b' they'): 0.11356924044156694, (b'is', b' the', b' best', b'-', b'known', b',', b' most', b' successful', b',', b' and'): 0.004418511235382777, (b'is', b' the', b' best', b'-', b'known', b' Canadian', b' team', b' to', b' have', b' won'): 0.000798666010624802}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticizing with a custom `Potential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom potential that does sentiment analysis.\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    ")\n",
    "\n",
    "\n",
    "class SentimentAnalysis(Potential):\n",
    "    def __init__(self, model, tokenizer, sentiment=\"POSITIVE\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.sentiment_idx = model.config.label2id.get(sentiment, None)\n",
    "        if self.sentiment_idx is None:\n",
    "            raise ValueError(f\"Sentiment {sentiment} not found in model labels\")\n",
    "\n",
    "        super().__init__(vocabulary=list(range(256)))  # Defined over bytes.\n",
    "\n",
    "    def _forward(self, contexts):\n",
    "        strings = [\n",
    "            bytes(context).decode(\"utf-8\", errors=\"ignore\") for context in contexts\n",
    "        ]  # Convert bytes to strings.\n",
    "        inputs = self.tokenizer(strings, return_tensors=\"pt\", padding=True)  # Tokenize.\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        return logits.log_softmax(dim=-1)[:, self.sentiment_idx].cpu().numpy()\n",
    "\n",
    "    async def prefix(self, context):\n",
    "        return self._forward([context])[0].item()\n",
    "\n",
    "    async def complete(self, context):\n",
    "        return self._forward([context])[0].item()\n",
    "\n",
    "    async def batch_complete(self, contexts):\n",
    "        return self._forward(contexts)\n",
    "\n",
    "    async def batch_prefix(self, contexts):\n",
    "        return self._forward(contexts)\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "sentiment_analysis = SentimentAnalysis(\n",
    "    model=DistilBertForSequenceClassification.from_pretrained(model_name),\n",
    "    tokenizer=DistilBertTokenizer.from_pretrained(model_name),\n",
    "    sentiment=\"POSITIVE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.00015841660206206143, -8.44865894317627)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await sentiment_analysis.prefix(b\"so good\"), await sentiment_analysis.prefix(b\"so bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that our custom potential satisfies the potential contract.\n",
    "await sentiment_analysis.assert_logw_next_consistency(b\"the best\", top=5)\n",
    "await sentiment_analysis.assert_autoreg_fact(b\"the best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is valid but will be slow!\n",
    "# slow_sampler = eager_token_sampler(\n",
    "#    iter_potential=product, item_potential=best_fsa * sentiment_analysis\n",
    "# )\n",
    "\n",
    "# This setup will be much faster.\n",
    "sampler = eager_token_sampler(product, best_fsa)\n",
    "critic = sentiment_analysis.coerce(sampler.target, f=b\"\".join)\n",
    "engine = InferenceEngine(sampler, critic=critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = await engine(n_particles=10, max_tokens=10, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&#x27; have&#x27;)</pre></td><td><pre>0.5140855398885724</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&#x27; are&#x27;)</pre></td><td><pre>0.1285215094145529</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&quot;&#x27;ve&quot;)</pre></td><td><pre>0.1285213259135228</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; but&#x27;, b&#x27; they&#x27;)</pre></td><td><pre>0.11940118987790568</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;, b&#x27; they&#x27;)</pre></td><td><pre>0.10269826874282272</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27;-&#x27;, b&#x27;known&#x27;, b&#x27; of&#x27;, b&#x27; the&#x27;, b&#x27; three&#x27;, b&#x27;.&#x27;, b&#x27; He&#x27;)</pre></td><td><pre>0.004838426304815362</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27;-&#x27;, b&#x27;known&#x27;, b&#x27; and&#x27;, b&#x27; most&#x27;, b&#x27; popular&#x27;, b&#x27; city&#x27;, b&#x27; in&#x27;)</pre></td><td><pre>0.001933739857808242</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{(b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b' have'): 0.5140855398885724, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b' are'): 0.1285215094145529, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b\"'ve\"): 0.1285213259135228, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' but', b' they'): 0.11940118987790568, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' and', b' they'): 0.10269826874282272, (b'is', b' the', b' best', b'-', b'known', b' of', b' the', b' three', b'.', b' He'): 0.004838426304815362, (b'is', b' the', b' best', b'-', b'known', b' and', b' most', b' popular', b' city', b' in'): 0.001933739857808242}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing with autobatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a new potential that automatically batches concurrent\n",
    "# requests to the instance methods (`prefix`, `complete`, `logw_next`)\n",
    "# and processes them using the batch methods (`batch_complete`, `batch_prefix`, `batch_logw_next`).\n",
    "critic = critic.to_autobatched()\n",
    "engine = InferenceEngine(sampler, critic=critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = await engine(n_particles=10, max_tokens=10, ess_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Monospace;\"><table><tr style=\"font-weight: bold;\"><td>key</td><td>value</td></tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&quot;&#x27;re&quot;)</pre></td><td><pre>0.25002511729300825</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; The&#x27;, b&#x27; team&#x27;)</pre></td><td><pre>0.15136989929033556</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&#x27; are&#x27;)</pre></td><td><pre>0.12501213401630581</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;.&#x27;, b&#x27; They&#x27;, b&#x27; have&#x27;)</pre></td><td><pre>0.12501200827576064</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; but&#x27;, b&#x27; the&#x27;)</pre></td><td><pre>0.11615715275068081</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;, b&#x27; they&#x27;)</pre></td><td><pre>0.09989401368109278</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; team&#x27;, b&#x27; in&#x27;, b&#x27; the&#x27;, b&#x27; league&#x27;, b&#x27;,&#x27;, b&#x27; and&#x27;, b&#x27; the&#x27;)</pre></td><td><pre>0.09989362075757438</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27;-&#x27;, b&#x27;known&#x27;, b&#x27; and&#x27;, b&#x27; most&#x27;, b&#x27; famous&#x27;, b&#x27; of&#x27;, b&#x27; the&#x27;)</pre></td><td><pre>0.017487806312315345</pre></td> </tr><tr><td><pre>(b&#x27;is&#x27;, b&#x27; the&#x27;, b&#x27; best&#x27;, b&#x27; place&#x27;, b&#x27; to&#x27;, b&#x27; get&#x27;, b&#x27; a&#x27;, b&#x27; good&#x27;, b&#x27; view&#x27;, b&#x27; of&#x27;)</pre></td><td><pre>0.015148247622926587</pre></td> </tr></table></div>"
      ],
      "text/plain": [
       "{(b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b\"'re\"): 0.25002511729300825, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' The', b' team'): 0.15136989929033556, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b' are'): 0.12501213401630581, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b'.', b' They', b' have'): 0.12501200827576064, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' but', b' the'): 0.11615715275068081, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' and', b' they'): 0.09989401368109278, (b'is', b' the', b' best', b' team', b' in', b' the', b' league', b',', b' and', b' the'): 0.09989362075757438, (b'is', b' the', b' best', b'-', b'known', b' and', b' most', b' famous', b' of', b' the'): 0.017487806312315345, (b'is', b' the', b' best', b' place', b' to', b' get', b' a', b' good', b' view', b' of'): 0.015148247622926587}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.posterior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
